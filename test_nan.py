"""
NaN ê°’ ë¹„ìœ¨ í™•ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python test_nan.py --dir_path /path/to/output_dump_batch
"""

import os
import glob
import torch
import numpy as np
import argparse
import time
import json
from collections import defaultdict
from tqdm import tqdm


def check_nan_in_tensor(tensor, name="tensor"):
    """í…ì„œì—ì„œ NaN ê°’ í™•ì¸"""
    if tensor is None:
        return {"has_nan": False, "nan_count": 0, "total_count": 0, "nan_ratio": 0.0}
    
    if isinstance(tensor, (int, float)):
        is_nan = np.isnan(tensor)
        return {
            "has_nan": is_nan,
            "nan_count": 1 if is_nan else 0,
            "total_count": 1,
            "nan_ratio": 1.0 if is_nan else 0.0
        }
    
    # Convert to numpy if tensor
    if hasattr(tensor, 'numpy'):
        arr = tensor.numpy()
    elif hasattr(tensor, 'get'):  # CuPy array
        arr = tensor.get()
    else:
        arr = np.asarray(tensor)
    
    nan_mask = np.isnan(arr)
    nan_count = np.sum(nan_mask)
    total_count = arr.size
    
    return {
        "has_nan": nan_count > 0,
        "nan_count": int(nan_count),
        "total_count": int(total_count),
        "nan_ratio": float(nan_count / total_count) if total_count > 0 else 0.0
    }


def analyze_single_file(file_path):
    """ë‹¨ì¼ .pt íŒŒì¼ì˜ NaN ë¶„ì„"""
    try:
        data = torch.load(file_path, weights_only=False)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None
    
    results = {}
    
    # í™•ì¸í•  í‚¤ ëª©ë¡
    keys_to_check = [
        # Energy ê´€ë ¨
        "calc_energy", "pred_energy", "gt_energy",
        # Forces ê´€ë ¨  
        "forces", "calc_forces", "pred_forces", "gt_forces",
        # Hamiltonian ê´€ë ¨
        "hamiltonian", "pred_hamiltonian", "init_ham",
        # Density matrix ê´€ë ¨
        "density_matrix",
        # Overlap ê´€ë ¨
        "overlap",
        # MO ê´€ë ¨
        "calc_mo_energy", "calc_mo_coeff", "mo_occ"
    ]
    
    for key in keys_to_check:
        if key in data:
            results[key] = check_nan_in_tensor(data[key], key)
        else:
            results[key] = None  # Key doesn't exist
    
    return results


def format_time(seconds):
    """ì´ˆë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if seconds < 60:
        return f"{seconds:.1f}ì´ˆ"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}ë¶„"
    else:
        hours = seconds / 3600
        return f"{hours:.2f}ì‹œê°„"


def analyze_directory(dir_path, pred_prefix="pred_", gt_prefix="gt_", calc_prefix="calc_"):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ë¶„ì„"""
    
    total_start_time = time.time()
    
    # ë°ì´í„°ì…‹ ì´ë¦„ ì¶”ì¶œ (ê²½ë¡œì—ì„œ -2 ìœ„ì¹˜, ì˜ˆ: /nas/.../malondialdehyde/output_dump -> malondialdehyde)
    path_parts = [p for p in dir_path.rstrip('/').split('/') if p]
    dataset_name = path_parts[-2] if len(path_parts) >= 2 else path_parts[-1] if path_parts else "unknown"
    
    # ê° prefixë³„ë¡œ íŒŒì¼ ì°¾ê¸°
    pred_files = sorted(glob.glob(os.path.join(dir_path, f"{pred_prefix}*.pt")))
    gt_files = sorted(glob.glob(os.path.join(dir_path, f"{gt_prefix}*.pt")))
    calc_files = sorted(glob.glob(os.path.join(dir_path, f"{calc_prefix}*.pt")))
    
    print(f"\n{'='*80}")
    print(f"ğŸ“‚ ë°ì´í„°ì…‹: {dataset_name.upper()}")
    print(f"{'='*80}")
    print(f"NaN ë¶„ì„ ì‹œì‘: {dir_path}")
    print(f"ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    print(f"ë°œê²¬ëœ íŒŒì¼ ìˆ˜: pred={len(pred_files)}, gt={len(gt_files)}, calc={len(calc_files)}")
    
    # í†µê³„ ìˆ˜ì§‘
    stats = defaultdict(lambda: {"files_with_nan": 0, "total_files": 0, "total_nan_count": 0, "total_element_count": 0})
    nan_file_indices = defaultdict(list)
    
    all_files = []
    for f in pred_files:
        all_files.append(("pred", f))
    for f in gt_files:
        all_files.append(("gt", f))
    for f in calc_files:
        all_files.append(("calc", f))
    
    total_files = len(all_files)
    print(f"\nì´ {total_files}ê°œ íŒŒì¼ ë¶„ì„ ì¤‘...")
    
    # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
    pbar = tqdm(all_files, desc="íŒŒì¼ ë¶„ì„", unit="file", 
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
    
    for file_type, file_path in pbar:
        results = analyze_single_file(file_path)
        if results is None:
            continue
        
        file_idx = os.path.basename(file_path)
        pbar.set_postfix_str(f"í˜„ì¬: {file_idx[:20]}...")
        
        for key, result in results.items():
            if result is None:
                continue
            
            stat_key = f"{file_type}_{key}"
            stats[stat_key]["total_files"] += 1
            stats[stat_key]["total_element_count"] += result["total_count"]
            stats[stat_key]["total_nan_count"] += result["nan_count"]
            
            if result["has_nan"]:
                stats[stat_key]["files_with_nan"] += 1
                nan_file_indices[stat_key].append(file_idx)
    
    pbar.close()
    
    # ì´ ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_elapsed = time.time() - total_start_time
    avg_time_per_file = total_elapsed / total_files if total_files > 0 else 0
    
    print(f"\n{'='*80}")
    print(f"ë¶„ì„ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {format_time(total_elapsed)}")
    print(f"íŒŒì¼ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_file*1000:.2f}ms")
    print(f"ì²˜ë¦¬ ì†ë„: {total_files/total_elapsed:.1f} files/sec")
    print(f"{'='*80}")
    
    # íŒŒì¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    file_types = ["pred", "gt", "calc"]
    type_stats = {ft: {"total_files": 0, "files_with_any_nan": set(), "keys": {}} for ft in file_types}
    
    for key, s in stats.items():
        for ft in file_types:
            if key.startswith(f"{ft}_"):
                field_name = key[len(ft)+1:]  # Remove prefix
                type_stats[ft]["keys"][field_name] = s
                type_stats[ft]["total_files"] = max(type_stats[ft]["total_files"], s["total_files"])
                break
    
    # NaN íŒŒì¼ ì¸ë±ìŠ¤ë¡œë¶€í„° ê° íƒ€ì…ë³„ NaN íŒŒì¼ ìˆ˜ ê³„ì‚°
    for key, indices in nan_file_indices.items():
        for ft in file_types:
            if key.startswith(f"{ft}_"):
                type_stats[ft]["files_with_any_nan"].update(indices)
                break
    
    # ============ ì „ì²´ ìš”ì•½ ============
    print(f"\n{'='*80}")
    print(f"ğŸ“Š [{dataset_name.upper()}] ì „ì²´ NaN ìš”ì•½ (íŒŒì¼ íƒ€ì…ë³„)")
    print(f"{'='*80}")
    print(f"{'íŒŒì¼ íƒ€ì…':<15} {'NaN ìˆëŠ” íŒŒì¼':<20} {'ì´ íŒŒì¼ìˆ˜':<15} {'NaN ë¹„ìœ¨':<15}")
    print(f"{'-'*80}")
    
    for ft in file_types:
        ts = type_stats[ft]
        nan_files = len(ts["files_with_any_nan"])
        total = ts["total_files"]
        ratio = nan_files / total * 100 if total > 0 else 0
        status = "âš ï¸ " if nan_files > 0 else "âœ… "
        print(f"{status}{ft.upper():<13} {nan_files:<20} {total:<15} {ratio:.2f}%")
    
    # ============ ê° íƒ€ì…ë³„ ìƒì„¸ ============
    for ft in file_types:
        ts = type_stats[ft]
        if ts["total_files"] == 0:
            continue
            
        print(f"\n{'='*80}")
        print(f"ğŸ“ {ft.upper()} íŒŒì¼ ìƒì„¸ ë¶„ì„")
        print(f"{'='*80}")
        print(f"{'í•„ë“œëª…':<30} {'NaN íŒŒì¼ìˆ˜':<15} {'ì´ íŒŒì¼ìˆ˜':<12} {'NaN ë¹„ìœ¨':<12} {'ìƒíƒœ'}")
        print(f"{'-'*80}")
        
        has_nan_in_type = False
        for field_name in sorted(ts["keys"].keys()):
            s = ts["keys"][field_name]
            if s["total_files"] > 0:
                file_ratio = s["files_with_nan"] / s["total_files"] * 100
                status = "âš ï¸ NaN!" if s["files_with_nan"] > 0 else "âœ… OK"
                if s["files_with_nan"] > 0:
                    has_nan_in_type = True
                print(f"{field_name:<30} {s['files_with_nan']:<15} {s['total_files']:<12} {file_ratio:>6.2f}%      {status}")
        
        if not has_nan_in_type:
            print(f"\n  âœ… {ft.upper()} íŒŒì¼ì—ì„œ NaNì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    
    # ============ NaN íŒŒì¼ ëª©ë¡ ============
    any_nan_found = any(len(ts["files_with_any_nan"]) > 0 for ts in type_stats.values())
    
    if any_nan_found:
        print(f"\n{'='*80}")
        print("ğŸ” NaNì´ ë°œê²¬ëœ íŒŒì¼ ëª©ë¡ (íƒ€ì…ë³„, ìµœëŒ€ 10ê°œ)")
        print(f"{'='*80}")
        
        for ft in file_types:
            nan_files = sorted(type_stats[ft]["files_with_any_nan"])
            if nan_files:
                print(f"\n[{ft.upper()}] NaN íŒŒì¼ ({len(nan_files)}ê°œ):")
                for idx in nan_files[:10]:
                    print(f"  - {idx}")
                if len(nan_files) > 10:
                    print(f"  ... ê·¸ ì™¸ {len(nan_files) - 10}ê°œ íŒŒì¼")
        
        # ìƒì„¸: ì–´ë–¤ í•„ë“œì—ì„œ NaNì´ ë°œìƒí–ˆëŠ”ì§€
        print(f"\n{'='*80}")
        print("ğŸ”¬ NaN ë°œìƒ í•„ë“œë³„ ìƒì„¸")
        print(f"{'='*80}")
        for key, indices in sorted(nan_file_indices.items()):
            print(f"\n{key} ({len(indices)}ê°œ íŒŒì¼):")
            for idx in indices[:5]:
                print(f"  - {idx}")
            if len(indices) > 5:
                print(f"  ... ê·¸ ì™¸ {len(indices) - 5}ê°œ íŒŒì¼")
    else:
        print(f"\n{'='*80}")
        print(f"âœ… [{dataset_name.upper()}] ëª¨ë“  íŒŒì¼ì—ì„œ NaNì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print(f"{'='*80}")
    
    # ìµœì¢… ìš”ì•½ í•œ ì¤„
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ [{dataset_name.upper()}] ìµœì¢… ìš”ì•½")
    print(f"{'='*80}")
    total_nan_files = sum(len(ts["files_with_any_nan"]) for ts in type_stats.values())
    total_all_files = sum(ts["total_files"] for ts in type_stats.values())
    print(f"  - PRED: {len(type_stats['pred']['files_with_any_nan'])}/{type_stats['pred']['total_files']} íŒŒì¼ì—ì„œ NaN ë°œê²¬ ({len(type_stats['pred']['files_with_any_nan'])/type_stats['pred']['total_files']*100 if type_stats['pred']['total_files'] > 0 else 0:.2f}%)")
    print(f"  - GT:   {len(type_stats['gt']['files_with_any_nan'])}/{type_stats['gt']['total_files']} íŒŒì¼ì—ì„œ NaN ë°œê²¬ ({len(type_stats['gt']['files_with_any_nan'])/type_stats['gt']['total_files']*100 if type_stats['gt']['total_files'] > 0 else 0:.2f}%)")
    print(f"  - CALC: {len(type_stats['calc']['files_with_any_nan'])}/{type_stats['calc']['total_files']} íŒŒì¼ì—ì„œ NaN ë°œê²¬ ({len(type_stats['calc']['files_with_any_nan'])/type_stats['calc']['total_files']*100 if type_stats['calc']['total_files'] > 0 else 0:.2f}%)")
    print(f"{'='*80}")
    
    return stats, nan_file_indices, type_stats, dataset_name, total_elapsed


def save_results_to_json(stats, nan_file_indices, type_stats, dataset_name, elapsed_time, dir_path):
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (md17_evaluation_customv2.py ìŠ¤íƒ€ì¼)"""
    
    # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    json_results = {
        "dataset_name": dataset_name,
        "analysis_timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
        "elapsed_time_seconds": elapsed_time,
        "summary": {},
        "detailed_stats": {},
        "nan_file_indices": {}
    }
    
    # íŒŒì¼ íƒ€ì…ë³„ ìš”ì•½
    file_types = ["pred", "gt", "calc"]
    for ft in file_types:
        ts = type_stats[ft]
        nan_files_count = len(ts["files_with_any_nan"])
        total_files = ts["total_files"]
        json_results["summary"][ft] = {
            "nan_files_count": nan_files_count,
            "total_files": total_files,
            "nan_ratio_percent": nan_files_count / total_files * 100 if total_files > 0 else 0,
            "nan_file_list": sorted(list(ts["files_with_any_nan"]))
        }
    
    # ìƒì„¸ í†µê³„ (statsë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜)
    for key, s in stats.items():
        json_results["detailed_stats"][key] = {
            "files_with_nan": s["files_with_nan"],
            "total_files": s["total_files"],
            "total_nan_count": s["total_nan_count"],
            "total_element_count": s["total_element_count"],
            "nan_ratio_percent": s["files_with_nan"] / s["total_files"] * 100 if s["total_files"] > 0 else 0
        }
    
    # NaN íŒŒì¼ ì¸ë±ìŠ¤ (ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
    for key, indices in nan_file_indices.items():
        json_results["nan_file_indices"][key] = sorted(indices)
    
    # md17_evaluation_customv2.py ìŠ¤íƒ€ì¼ë¡œ ì €ì¥
    # dataset_name = dir_path.split("/")[-2]
    os.makedirs('./outputs2', exist_ok=True)
    output_file = os.path.join('./outputs2', f"{dataset_name}_nan_analysis.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, ensure_ascii=False, indent=4)
    
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="NaN ê°’ ë¹„ìœ¨ í™•ì¸ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--dir_path", type=str, required=True, 
                        help="ë¶„ì„í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: /nas/seongjun/sphnet/aspirin/output_dump_batch)")
    parser.add_argument("--pred_prefix", type=str, default="pred_")
    parser.add_argument("--gt_prefix", type=str, default="gt_")
    parser.add_argument("--calc_prefix", type=str, default="calc_")
    parser.add_argument("--single_file", type=str, default=None,
                        help="ë‹¨ì¼ íŒŒì¼ë§Œ ë¶„ì„í•˜ë ¤ë©´ íŒŒì¼ ê²½ë¡œ ì§€ì •")
    
    args = parser.parse_args()
    
    if args.single_file:
        print(f"\në‹¨ì¼ íŒŒì¼ ë¶„ì„: {args.single_file}")
        start_time = time.time()
        results = analyze_single_file(args.single_file)
        elapsed = time.time() - start_time
        if results:
            print(f"\n{'Key':<30} {'Has NaN':<10} {'NaN Count':<15} {'Total':<15} {'Ratio':<10}")
            print(f"{'-'*80}")
            for key, result in results.items():
                if result is not None:
                    print(f"{key:<30} {str(result['has_nan']):<10} {result['nan_count']:<15} {result['total_count']:<15} {result['nan_ratio']:.4f}")
            print(f"\nì†Œìš” ì‹œê°„: {elapsed*1000:.2f}ms")
    else:
        stats, nan_file_indices, type_stats, dataset_name, elapsed_time = analyze_directory(
            args.dir_path,
            pred_prefix=args.pred_prefix,
            gt_prefix=args.gt_prefix,
            calc_prefix=args.calc_prefix
        )
        
        # JSON íŒŒì¼ ì €ì¥ (md17_evaluation_customv2.py ìŠ¤íƒ€ì¼)
        save_results_to_json(stats, nan_file_indices, type_stats, dataset_name, elapsed_time, args.dir_path)


if __name__ == "__main__":
    main()

